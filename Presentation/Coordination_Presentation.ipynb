{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99120492-334c-443f-9a4b-d504b7efe4b9",
   "metadata": {},
   "source": [
    "# Description of the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3333c8-861c-4a2e-80d0-f7e270049549",
   "metadata": {},
   "source": [
    "Let's say we have a soccer robot on a field like the ones in bellow:\n",
    "\n",
    "<img src = \"./images/robots_on_the_field.jpg\" style=\"width:680px;height:500px;\">\n",
    "\n",
    "and each one can turn its head from side to side to get a view of its surroundings, in the form of multiple pictures like this:\n",
    "\n",
    "### <mark>some good picture!!!</mark>\n",
    "\n",
    "<!--- <img src = \"./images/camera_view.jpg\" style=\"width:680px;height:500px;\"> --->\n",
    "The question is \n",
    "\n",
    "> **How can we find the location of the robot based on the pictures it has got from its surroundings?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cfa5a7-c1cc-4c72-a624-0415a2b9ac5f",
   "metadata": {},
   "source": [
    "To dive a little bit further into the problem, here is a picture of the field which is ***8m $\\times$ 11m*** (the upper left corner is chosen as the origin, $x$ axis is directed downward, and the $y$ axis points to the right side of the field), and imagine that the robot is located on the <span style=\"color:red\">red</span> spot located at $(2, 4)$, i.e., $2m$ from the left end to the right and $4m$ below the upper end, and faced at $60$ degree angle counterclockwise from the left edge of the field:\n",
    "\n",
    "<img src = \"./images/an_instance.png\" style=\"width:680px;height:500px;\">\n",
    "\n",
    "Now when the robot turns its head from right to left, it takes a few pictures to get a view of what does its surroundings look like, and it sees something like this:\n",
    "\n",
    "<img src = \"./images/head_camera.png\" style=\"width:900px;height:500px;\">\n",
    "\n",
    "our main task is to\n",
    "\n",
    "> **Try to find the location of the robot on the field from what it sees.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6809d2b4-9d5b-4ead-9d1f-878b99e28b45",
   "metadata": {},
   "source": [
    "# The main idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12a829d-6e3d-40ec-a663-b187d89e59bd",
   "metadata": {},
   "source": [
    "Intuitively speaking, humans are capable of figuring out where they should be on the field to have a certain view, but how do we do it?\n",
    "\n",
    "We believe the answer to this question, consists of two main parts:\n",
    "1. Humans can learn **where things are relative to themselves** by looking at them, and\n",
    "2. They use this information to **compare it to their knowledge** of what a soccer field looks like, to find where they must be on the field.\n",
    "\n",
    "\n",
    "So we attempt to mimic the same principles. In short, our idea is to\n",
    "> use the **white stright lines** on the field to get a map of the portion of the field surrounding the robot, and then compare it with the full map of the field, which we have, to see where we are.\n",
    "\n",
    "so our path from here devides into two parts:\n",
    "1. get a **local map** of the portion of the field surrounding the robot, with respect to the robot itself\n",
    "2. **compare** this observed map with the original map to finde where the robot is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020bc86b-1f33-4549-8ed0-4fbfc43a412c",
   "metadata": {},
   "source": [
    "## getting the local map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069b500e-fd9c-40a2-b077-be4387055dbc",
   "metadata": {},
   "source": [
    "In order to do this, first we use the white color of the lines to get a black and white image to keep the white lines and throw away everything else, like this:\n",
    "<img src = \"./images/filtered_field.png\" style=\"width:680px;height:500px;\">\n",
    "\n",
    "But, how do humans learn about how far or close things are from themselves just by looking at them?\n",
    "\n",
    "The point is that\n",
    "\n",
    "> **humans have two eyes, so they can percieve depth**\n",
    "\n",
    "For the sake of simplicity, consider a 2D verson of this problem, lets say a person(with two eyes!) is looking at a <span style=\"color:red\">red</span> point on a wall(at distance $D$ from the person), exactly in front of him/her, and the point is $x$ meters to his/her left like below:\n",
    "\n",
    "<img src = \"./images/two_eyes.png\" style=\"width:680px;height:500px;\">\n",
    "\n",
    "In principle, the person can find both $x$ and $D$, only using his eye angles $\\phi_1$, $\\phi_2$ and knowing the distance $S$ between his/her eyes, and thus finding the position of the point reletive to him/herself.\n",
    "\n",
    "It would be nice if we could do something similar with our robot, but there is a tiny problem with that:\n",
    "> **due to our limeted budget, we could afford only one camera:)**\n",
    "\n",
    "So, we need to solve this problem, using **only one angle**.\n",
    "\n",
    "We recall that the wall in our analogy is actually the soccer field and\n",
    "> **the distance from the camera to the field is actually determined by the hight of the robot itself!**\n",
    "\n",
    "Hence we don't need to find that too, we can measure it directly beforehand hopefully assuming our robots will not shrink or grow any taller during the match:)\n",
    "\n",
    "Here's a depiction of what we mean by this:\n",
    "\n",
    "<img src = \"./images/one_eyes.png\" style=\"width:680px;height:500px;\">\n",
    "\n",
    "If we assume we know $D$, and are able to measure $\\pi$, we can have $x$ as:\n",
    "\n",
    "$$\\tan(\\phi) = \\frac{x}{D}\\\\\n",
    "\\implies x = D \\tan(\\phi)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2e6d63-4473-4a22-b413-6fe8022d9528",
   "metadata": {},
   "source": [
    "Now returning to our problem, let's say we see a <span style=\"color:blue\">blue</span> dot on our image as this:\n",
    "\n",
    "<img src = \"./images/blue_dot_on_camera_frame.png\" style=\"width:680px;height:500px;\">\n",
    "\n",
    "and we name its coordinates as $(y', z')$, but infact, this point on the camera image corresponds to a point on the field like this:\n",
    "\n",
    "<img src = \"./images/blue_dot_on_field_frame.png\" style=\"width:680px;height:500px;\">\n",
    "\n",
    "which we name its coordinates with respect to the robot(robot frame) as $(x, y)$.\n",
    "And we are curious about how can we get from $(y', z')$ to $(x, y)$?\n",
    "\n",
    "$$\\Huge{(y', z')\\xrightarrow{???} (x, y)}$$\n",
    "\n",
    "To be able to answer this, we're going to need to be more precise about the definition of our **robot frame**, i.e., \n",
    "> to which part of the robot is it attached exactly?\n",
    "\n",
    "So we need to talk a little about how our robot is assembled and controlled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da89956e-f123-4d44-a53c-67b4f52f992c",
   "metadata": {},
   "source": [
    "### schematic of the robot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0304b8b6-9deb-4249-8739-a73693b6dd5e",
   "metadata": {},
   "source": [
    "#### A simplified version of how the robot is assembled and how are its joints is presented bellow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83182940-21db-4c71-994f-a941d6f0b4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"1500\"\n",
       "            src=\"http://127.0.0.1:8051\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x753c94a2aad0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "# URL where your Dash app is hosted\n",
    "dash_app_url = \"http://127.0.0.1:8051\"  # Replace with your Dash app URL\n",
    "\n",
    "# Display the Dash app in an IFrame\n",
    "IFrame(src=dash_app_url, width=1000, height=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1de27c-1c63-476d-b2d4-d086353ad6cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee0888c-24fd-4d1b-aca0-4efcba4bc250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec9210c0-3779-4476-851b-b765d610f5c0",
   "metadata": {},
   "source": [
    "Our approach for this is to solve it in multiple steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a575abe-8a0f-4cc5-a520-ffb89145ab79",
   "metadata": {},
   "source": [
    "Now we use the image processing library **OpenCv** to get the lines on this filtered image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84c4a5e2-473d-45b2-9a97-22a550afce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import HTML\n",
    "\n",
    "#HTML(filename=\"./images/robot_sketch.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee92af6e-3d81-47db-8edd-aa0306fc9bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031f0540-5b7a-4a1f-9b38-d1cb1d3a545c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c17cb1-add7-4367-9709-b203fb2bedbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26446aa8-c5f8-461e-b3d3-33b17e5ee3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext watermark\n",
    "#%watermark -v -m -p watermark\n",
    "#%watermark -u -n -t -z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3297be22-b77c-4200-bacb-781c5ac47958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da470de-38ea-47a0-ad9b-271c4893a2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ec6284-b0f4-4563-8fc2-261484b3ff9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
